{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae182f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers, AutoTokenizer, torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190f9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11e6a57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33582a9e255546b0ae8a2bf7d83000b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00953ca92dab4c848231b07f602be0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b731e2814946d7b8b2df17843ef6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef37dc312a3b4218b3527481e5c062b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3939c4aca58449f58dbbea73c382d9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8b6b4edcc14c91832e25dc30eea84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201e4a33b5a64bef84cb8dbadc658383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in dataset: 36718\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a text dataset (we use a small example dataset for demonstration)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")  # raw text WikiText-2\n",
    "print(f\"Number of lines in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb52a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed529eadcdb46ee91c111de4eb805ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8180ce64851141f3b3f011988292ae28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83ada324dee404094e3388b5c99d8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b0366a926246c79f73b911a9cc8594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd087d9443248f684683ea43c862dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Initialize a tokenizer (we'll use GPT-2's tokenizer for compatibility with a GPT-2 model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd45b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ea6d27fa57446b923bacdf560d09a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 102]\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenize the dataset efficiently using `.map` with batched processing\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=False)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# The dataset now has columns like 'input_ids' and 'attention_mask'\n",
    "\n",
    "print(tokenized_ds[0][\"input_ids\"][:20])  # print first 20 token IDs of first example for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dee6076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in dataset: 36718\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "print(f\"Number of lines in dataset: {len(dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d832435b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a3060a3a584a66a9d840b2f2302d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False  # IMPORTANT\n",
    "    )\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True)\n",
    "print(\"Tokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e369dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate ALL tokens\n",
    "    concatenated_ids = sum(examples[\"input_ids\"], [])\n",
    "    concatenated_mask = sum(examples[\"attention_mask\"], [])\n",
    "\n",
    "    # Round down to nearest block\n",
    "    total_len = (len(concatenated_ids) // block_size) * block_size\n",
    "    concatenated_ids = concatenated_ids[:total_len]\n",
    "    concatenated_mask = concatenated_mask[:total_len]\n",
    "\n",
    "    # Split into chunks\n",
    "    ids_chunks = [\n",
    "        concatenated_ids[i:i + block_size]\n",
    "        for i in range(0, total_len, block_size)\n",
    "    ]\n",
    "    mask_chunks = [\n",
    "        concatenated_mask[i:i + block_size]\n",
    "        for i in range(0, total_len, block_size)\n",
    "    ]\n",
    "\n",
    "    # MUST RETURN FLAT LIST OF CHUNKS\n",
    "    return {\n",
    "        \"input_ids\": ids_chunks,\n",
    "        \"attention_mask\": mask_chunks\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9905908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed85283001064a39b21e2f54c002ed4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM training sequences: 9260\n"
     ]
    }
   ],
   "source": [
    "lm_ds = tokenized_ds.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=tokenized_ds.column_names\n",
    ")\n",
    "\n",
    "print(\"LM training sequences:\", len(lm_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f2c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate\n",
    "    concatenated = []\n",
    "    for ids in examples[\"input_ids\"]:\n",
    "        concatenated.extend(ids)\n",
    "\n",
    "    total_len = (len(concatenated) // block_size) * block_size\n",
    "    concatenated = concatenated[:total_len]\n",
    "\n",
    "    # Split\n",
    "    chunks = [\n",
    "        concatenated[i:i + block_size]\n",
    "        for i in range(0, total_len, block_size)\n",
    "    ]\n",
    "\n",
    "    return {\"input_ids\": chunks}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf6f9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95711f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe849645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a5f9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Wrap HF dataset to use with PyTorch DataLoader\n",
    "class HFDatasetWrapper(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.ds = hf_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ds[idx]\n",
    "\n",
    "wrapped_ds = HFDatasetWrapper(lm_ds)\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in batch], dtype=torch.long)\n",
    "    return {\"input_ids\": input_ids, \"labels\": input_ids.clone()}\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    wrapped_ds,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5edc38fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256]) torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. Iterate through a couple of batches to see that it works\n",
    "for batch in train_dataloader:\n",
    "    print(batch[\"input_ids\"].shape, batch[\"labels\"].shape)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
